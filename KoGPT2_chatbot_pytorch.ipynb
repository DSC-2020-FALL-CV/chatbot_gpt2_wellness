{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOqL-ERxwMDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "736237e4-5772-4a0a-d628-eca21a883317"
      },
      "source": [
        "# GPU 정보 \n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 25 10:55:47 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUbPw_7Bf9Jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7dcd19a4-b6ed-4bb1-f844-7d2e419abf50"
      },
      "source": [
        "# 현재 CUDA Version에 맞는 Pytorch 설치\n",
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WKGlGahhDAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "306c7476-a021-48f0-ce0b-c3a85bffd2a4"
      },
      "source": [
        "# 패키지 설치\n",
        "!pip install mxnet==1.6.0 gluonnlp==0.9.1 sentencepiece==0.1.91 pandas==1.0.5 transformers==2.11.0 pytorch_lightning==0.8.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 43kB/s \n",
            "\u001b[?25hCollecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 37.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 39.2MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/8e/d1bb6f3696aaed40bf8263c0d9be95593dbc6a63921cca68f0e7f60e7893/pytorch_lightning-0.8.1-py3-none-any.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 33.7MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 33.5MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.5.1+cu101)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (47.3.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.6.0.post3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.29.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.4.8)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses, future, PyYAML\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470038 sha256=365d143c568541d9aa8745bf1183f2377edfd391f0d801b779d0c59d81203d50\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5bdd4e1c3d42ea5c30d114f624d7c0ba5c06fc3a032b73c9db92a69b13547387\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=faa0531e81dd05ef7953578aba9a346e1d847032ed8da84a3f4acca263bc7e96\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=a21ab9d678fe198044eef0752eb978e2c464f890a3b56a0ab875821d713ccc67\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built gluonnlp sacremoses future PyYAML\n",
            "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, tokenizers, sacremoses, transformers, future, PyYAML, pytorch-lightning\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 future-0.18.2 gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 pytorch-lightning-0.8.1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc9gWX3SkP6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "b79d143d-908a-4914-b47d-bd0f4a3cfa86"
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoGPT2#egg=kogpt2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kogpt2\n",
            "  Cloning https://github.com/SKT-AI/KoGPT2 to /tmp/pip-install-lmf90hij/kogpt2\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoGPT2 /tmp/pip-install-lmf90hij/kogpt2\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14052 sha256=241778ebcd2f58850e8278397b4d0132116b43e1deb6eb5691021367647a3e25\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-os9az673/wheels/2a/9f/62/3cba71a35387ff5da1d12e6b053b4d839dab0ed4310dde840d\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8W3gZk2ijYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "bc4ee445-1bf3-4af2-c433-375634918c7c"
      },
      "source": [
        "# KoGPT2-chatbot 소스 코드 복사\n",
        "!git clone --recurse-submodules https://github.com/haven-jeon/KoGPT2-chatbot.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoGPT2-chatbot'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/62)\u001b[K\rremote: Counting objects:   3% (2/62)\u001b[K\rremote: Counting objects:   4% (3/62)\u001b[K\rremote: Counting objects:   6% (4/62)\u001b[K\rremote: Counting objects:   8% (5/62)\u001b[K\rremote: Counting objects:   9% (6/62)\u001b[K\rremote: Counting objects:  11% (7/62)\u001b[K\rremote: Counting objects:  12% (8/62)\u001b[K\rremote: Counting objects:  14% (9/62)\u001b[K\rremote: Counting objects:  16% (10/62)\u001b[K\rremote: Counting objects:  17% (11/62)\u001b[K\rremote: Counting objects:  19% (12/62)\u001b[K\rremote: Counting objects:  20% (13/62)\u001b[K\rremote: Counting objects:  22% (14/62)\u001b[K\rremote: Counting objects:  24% (15/62)\u001b[K\rremote: Counting objects:  25% (16/62)\u001b[K\rremote: Counting objects:  27% (17/62)\u001b[K\rremote: Counting objects:  29% (18/62)\u001b[K\rremote: Counting objects:  30% (19/62)\u001b[K\rremote: Counting objects:  32% (20/62)\u001b[K\rremote: Counting objects:  33% (21/62)\u001b[K\rremote: Counting objects:  35% (22/62)\u001b[K\rremote: Counting objects:  37% (23/62)\u001b[K\rremote: Counting objects:  38% (24/62)\u001b[K\rremote: Counting objects:  40% (25/62)\u001b[K\rremote: Counting objects:  41% (26/62)\u001b[K\rremote: Counting objects:  43% (27/62)\u001b[K\rremote: Counting objects:  45% (28/62)\u001b[K\rremote: Counting objects:  46% (29/62)\u001b[K\rremote: Counting objects:  48% (30/62)\u001b[K\rremote: Counting objects:  50% (31/62)\u001b[K\rremote: Counting objects:  51% (32/62)\u001b[K\rremote: Counting objects:  53% (33/62)\u001b[K\rremote: Counting objects:  54% (34/62)\u001b[K\rremote: Counting objects:  56% (35/62)\u001b[K\rremote: Counting objects:  58% (36/62)\u001b[K\rremote: Counting objects:  59% (37/62)\u001b[K\rremote: Counting objects:  61% (38/62)\u001b[K\rremote: Counting objects:  62% (39/62)\u001b[K\rremote: Counting objects:  64% (40/62)\u001b[K\rremote: Counting objects:  66% (41/62)\u001b[K\rremote: Counting objects:  67% (42/62)\u001b[K\rremote: Counting objects:  69% (43/62)\u001b[K\rremote: Counting objects:  70% (44/62)\u001b[K\rremote: Counting objects:  72% (45/62)\u001b[K\rremote: Counting objects:  74% (46/62)\u001b[K\rremote: Counting objects:  75% (47/62)\u001b[K\rremote: Counting objects:  77% (48/62)\u001b[K\rremote: Counting objects:  79% (49/62)\u001b[K\rremote: Counting objects:  80% (50/62)\u001b[K\rremote: Counting objects:  82% (51/62)\u001b[K\rremote: Counting objects:  83% (52/62)\u001b[K\rremote: Counting objects:  85% (53/62)\u001b[K\rremote: Counting objects:  87% (54/62)\u001b[K\rremote: Counting objects:  88% (55/62)\u001b[K\rremote: Counting objects:  90% (56/62)\u001b[K\rremote: Counting objects:  91% (57/62)\u001b[K\rremote: Counting objects:  93% (58/62)\u001b[K\rremote: Counting objects:  95% (59/62)\u001b[K\rremote: Counting objects:  96% (60/62)\u001b[K\rremote: Counting objects:  98% (61/62)\u001b[K\rremote: Counting objects: 100% (62/62)\u001b[K\rremote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/44)\u001b[K\rremote: Compressing objects:   4% (2/44)\u001b[K\rremote: Compressing objects:   6% (3/44)\u001b[K\rremote: Compressing objects:   9% (4/44)\u001b[K\rremote: Compressing objects:  11% (5/44)\u001b[K\rremote: Compressing objects:  13% (6/44)\u001b[K\rremote: Compressing objects:  15% (7/44)\u001b[K\rremote: Compressing objects:  18% (8/44)\u001b[K\rremote: Compressing objects:  20% (9/44)\u001b[K\rremote: Compressing objects:  22% (10/44)\u001b[K\rremote: Compressing objects:  25% (11/44)\u001b[K\rremote: Compressing objects:  27% (12/44)\u001b[K\rremote: Compressing objects:  29% (13/44)\u001b[K\rremote: Compressing objects:  31% (14/44)\u001b[K\rremote: Compressing objects:  34% (15/44)\u001b[K\rremote: Compressing objects:  36% (16/44)\u001b[K\rremote: Compressing objects:  38% (17/44)\u001b[K\rremote: Compressing objects:  40% (18/44)\u001b[K\rremote: Compressing objects:  43% (19/44)\u001b[K\rremote: Compressing objects:  45% (20/44)\u001b[K\rremote: Compressing objects:  47% (21/44)\u001b[K\rremote: Compressing objects:  50% (22/44)\u001b[K\rremote: Compressing objects:  52% (23/44)\u001b[K\rremote: Compressing objects:  54% (24/44)\u001b[K\rremote: Compressing objects:  56% (25/44)\u001b[K\rremote: Compressing objects:  59% (26/44)\u001b[K\rremote: Compressing objects:  61% (27/44)\u001b[K\rremote: Compressing objects:  63% (28/44)\u001b[K\rremote: Compressing objects:  65% (29/44)\u001b[K\rremote: Compressing objects:  68% (30/44)\u001b[K\rremote: Compressing objects:  70% (31/44)\u001b[K\rremote: Compressing objects:  72% (32/44)\u001b[K\rremote: Compressing objects:  75% (33/44)\u001b[K\rremote: Compressing objects:  77% (34/44)\u001b[K\rremote: Compressing objects:  79% (35/44)\u001b[K\rremote: Compressing objects:  81% (36/44)\u001b[K\rremote: Compressing objects:  84% (37/44)\u001b[K\rremote: Compressing objects:  86% (38/44)\u001b[K\rremote: Compressing objects:  88% (39/44)\u001b[K\rremote: Compressing objects:  90% (40/44)\u001b[K\rremote: Compressing objects:  93% (41/44)\u001b[K\rremote: Compressing objects:  95% (42/44)\u001b[K\rremote: Compressing objects:  97% (43/44)\u001b[K\rremote: Compressing objects: 100% (44/44)\u001b[K\rremote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "Unpacking objects:   1% (1/62)   \rUnpacking objects:   3% (2/62)   \rUnpacking objects:   4% (3/62)   \rUnpacking objects:   6% (4/62)   \rUnpacking objects:   8% (5/62)   \rUnpacking objects:   9% (6/62)   \rUnpacking objects:  11% (7/62)   \rUnpacking objects:  12% (8/62)   \rUnpacking objects:  14% (9/62)   \rUnpacking objects:  16% (10/62)   \rUnpacking objects:  17% (11/62)   \rUnpacking objects:  19% (12/62)   \rUnpacking objects:  20% (13/62)   \rUnpacking objects:  22% (14/62)   \rUnpacking objects:  24% (15/62)   \rUnpacking objects:  25% (16/62)   \rUnpacking objects:  27% (17/62)   \rUnpacking objects:  29% (18/62)   \rUnpacking objects:  30% (19/62)   \rUnpacking objects:  32% (20/62)   \rUnpacking objects:  33% (21/62)   \rUnpacking objects:  35% (22/62)   \rUnpacking objects:  37% (23/62)   \rUnpacking objects:  38% (24/62)   \rUnpacking objects:  40% (25/62)   \rUnpacking objects:  41% (26/62)   \rremote: Total 62 (delta 31), reused 40 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects:  43% (27/62)   \rUnpacking objects:  45% (28/62)   \rUnpacking objects:  46% (29/62)   \rUnpacking objects:  48% (30/62)   \rUnpacking objects:  50% (31/62)   \rUnpacking objects:  51% (32/62)   \rUnpacking objects:  53% (33/62)   \rUnpacking objects:  54% (34/62)   \rUnpacking objects:  56% (35/62)   \rUnpacking objects:  58% (36/62)   \rUnpacking objects:  59% (37/62)   \rUnpacking objects:  61% (38/62)   \rUnpacking objects:  62% (39/62)   \rUnpacking objects:  64% (40/62)   \rUnpacking objects:  66% (41/62)   \rUnpacking objects:  67% (42/62)   \rUnpacking objects:  69% (43/62)   \rUnpacking objects:  70% (44/62)   \rUnpacking objects:  72% (45/62)   \rUnpacking objects:  74% (46/62)   \rUnpacking objects:  75% (47/62)   \rUnpacking objects:  77% (48/62)   \rUnpacking objects:  79% (49/62)   \rUnpacking objects:  80% (50/62)   \rUnpacking objects:  82% (51/62)   \rUnpacking objects:  83% (52/62)   \rUnpacking objects:  85% (53/62)   \rUnpacking objects:  87% (54/62)   \rUnpacking objects:  88% (55/62)   \rUnpacking objects:  90% (56/62)   \rUnpacking objects:  91% (57/62)   \rUnpacking objects:  93% (58/62)   \rUnpacking objects:  95% (59/62)   \rUnpacking objects:  96% (60/62)   \rUnpacking objects:  98% (61/62)   \rUnpacking objects: 100% (62/62)   \rUnpacking objects: 100% (62/62), done.\n",
            "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
            "Cloning into '/content/KoGPT2-chatbot/Chatbot_data'...\n",
            "remote: Enumerating objects: 20, done.        \n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20        \n",
            "Submodule path 'Chatbot_data': checked out '235fac5aea3badab22743f7048afe936cf72f822'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ZweKmXiuaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cc6ce8b-105b-4f39-b865-22508865cf6c"
      },
      "source": [
        "# 폴더 이동\n",
        "%cd KoGPT2-chatbot"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2-chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMZv-ZsiqkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "7cf55491-8fbd-4201-a849-9616b638d3d5"
      },
      "source": [
        "# 사전훈련된 KoGPT2를 챗봇 데이터로 파인튜닝\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --train --gpus 1 --max_epochs 2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-25 10:56:43.124599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "using cached model\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 124 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 2 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Epoch 1:   0% 0/185 [00:00<?, ?it/s] INFO:root:contexts : 다가온 이별\n",
            "INFO:root:toked ctx: ['<usr>', '▁다가온', '▁이별', '</s>', '<unused1>', '▁1', '</s>']\n",
            "INFO:root:response : 이별 예약제.\n",
            "INFO:root:contexts : 행복하기 위해 중요한 게 뭐야?\n",
            "INFO:root:toked response : ['<sys>', '▁이별', '▁예약', '제', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁이별', '▁예약', '제', '.', '</s>']\n",
            "INFO:root:toked ctx: ['<usr>', '▁행복', '하기', '▁위해', '▁중요한', '▁게', '▁뭐', '야', '?', '</s>', '<unused1>', '▁0', '</s>']\n",
            "INFO:root:response : 인생을 즐기는 거죠.\n",
            "INFO:root:toked response : ['<sys>', '▁인생을', '▁즐기는', '▁거죠', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁인생을', '▁즐기는', '▁거죠', '.', '</s>']\n",
            "Epoch 1: 100% 185/185 [03:54<00:00,  1.27s/it, loss=2.400, v_num=0]\n",
            "Epoch 00001: loss reached 2.51730 (best 2.51730), saving model to /content/KoGPT2-chatbot/model_chp/model_epoch=01-loss=2.52.ckpt as top 1\n",
            "Epoch 2:   0% 0/185 [00:00<?, ?it/s, loss=2.400, v_num=0]INFO:root:contexts : 사랑하는 사람은 딱 알아볼 수 있어?\n",
            "INFO:root:toked ctx: ['<usr>', '▁사랑하는', '▁사람은', '▁딱', '▁알아볼', '▁수', '▁있어', '?', '</s>', '<unused1>', '▁2', '</s>']\n",
            "INFO:root:response : 감정은 감출수 있는게 아니에요.\n",
            "INFO:root:toked response : ['<sys>', '▁감', '정은', '▁감', '출', '수', '▁있는', '게', '▁아니', '에요', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁감', '정은', '▁감', '출', '수', '▁있는', '게', '▁아니', '에요', '.', '</s>']\n",
            "INFO:root:contexts : 이미 잘못이 너무 많아서.\n",
            "INFO:root:toked ctx: ['<usr>', '▁이미', '▁잘못', '이', '▁너무', '▁많아서', '.', '</s>', '<unused1>', '▁1', '</s>']\n",
            "INFO:root:response : 더 이상의 잘못은 이해하지 못할 거예요.\n",
            "INFO:root:toked response : ['<sys>', '▁더', '▁이상의', '▁잘못', '은', '▁이해하지', '▁못할', '▁거예요', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁더', '▁이상의', '▁잘못', '은', '▁이해하지', '▁못할', '▁거예요', '.', '</s>']\n",
            "Epoch 2: 100% 185/185 [03:58<00:00,  1.29s/it, loss=2.078, v_num=0]\n",
            "Epoch 00002: loss reached 2.17900 (best 2.17900), saving model to /content/KoGPT2-chatbot/model_chp/model_epoch=02-loss=2.18.ckpt as top 1\n",
            "Epoch 2: 100% 185/185 [04:42<00:00,  1.52s/it, loss=2.078, v_num=0]\n",
            "INFO:root:best model path /content/KoGPT2-chatbot/model_chp/model_epoch=02-loss=2.18.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3yDcidi6wFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "b66e761c-3731-4f82-ce8b-411a65a7363d"
      },
      "source": [
        "# 대화 테스트, `quit`를 입력하면 대화를 종료합니다.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --chat"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-25 11:07:27.562722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "user > 집이 너무 비싸네.\n",
            "Simsimi > 집 없는 설움을 겪으셨나봐요.\n",
            "user > 아침에 잠이 너무 많아서 걱정이야.\n",
            "Simsimi > 잠이 안 올 거예요.\n",
            "user > 비도 많이 오고 너무 더워.\n",
            "Simsimi > 시원한 음료 드세요.\n",
            "user > 비오는데 우산을 안가져 왔네..\n",
            "Simsimi > 우산 챙기세요.\n",
            "user > 비가 너무 많이와\n",
            "Simsimi > 우산 챙기세요.\n",
            "user > quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoeGog7OoXFk",
        "colab_type": "text"
      },
      "source": [
        "이 노트북은   https://colab.research.google.com/drive/1Np7d8zrch589LwwW9oX_MyzJ9jfPEvUG?usp=sharing  를 수정하여 만들었습니다..  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "KoGPT2_chatbot_pytorch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}